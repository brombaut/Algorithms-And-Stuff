demo()
x = c(1, 2, 3)
x
y = c(5, 3, 4)
length(y)
x+y
ls()
?matrix
x = 1:10
y = x
f = outer(x, y, function(x, y) cos(y)/(1+x^2))
contour(x, y, f)
contour(x, y, f, nlevels=45, add=T)
fa = (f-t(f))/2
contour(x, y, fa, nlevels=15)
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta=30)
persp(x, y, fa, theta=30, phi=20)
persp(x, y, fa, theta=30, phi=70)
persp(x, y, fa, theta=30, phi=40)
A = matrix(1:16, 4, 4)
install.packages(c("boot", "class", "cluster", "KernSmooth", "lattice", "MASS", "Matrix", "mgcv", "nlme", "nnet", "rpart", "spatial", "survival"))
install.packages("ISLR")
library(ISLR)
library(MASS)
exit()
exit
pairs(Auto)
library(ISLR)
pairs(Auto)
clear
pwd
x <- c(1, 2, 3)
c
x
ls()
rm(list=ls())
ls()
?matrix
?c
x = matrix(data=c(1,2,3,4), nrow=2, ncol=2)
x
x = matrix(data=c(1,2,3,4), nrow=2, ncol=2, byrow = TRUE)
x
y = sqrt(x)
y
x ^ 2
x = rnorm(50)
x
y = x + rnorm(50, mean=50, sd = 1)
y
cor(x, y)
y = x + rnorm(50, mean=50, sd = 0.1)
cor(x, y)
set.seed(1303)
rnorm(50)
clear
clear()
x = rnorm(100)
y = rnorm(100)
plot(x, y)
x = seq(-pi, pi, length=50)
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f, nlevels = 45, add=T)
fa=(f-t(f))/2
contour(x,y,fa, nlevels = 15)
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa, theta = 30)
library(MASS)
library(ISLR)
install.packages("ISLR")
library(ISLR)
?Auto
fix(Auto)
fix(Auto)
fix(Boston)
fix(Auto)
?Auto
library(ISLR)
library(MASS, lib.loc = "/usr/local/Cellar/r/4.0.2_1/lib/R/library")
?Auto
fix(Auto)
fix(Boston)
fix(Auto)
pairs(Auto)
names(Auto)
library(ISLR)
library(MASS)
?Auto
fix(Auto)
names(Auto)
?fix
edit(Auto)
?View
View(Auto)
dim(Auto)
Auto[1:4,]
Auto = na.omit(Auto)
dim(Auto)
names(Auto)
plot(cylinders, mpg)
plot(Auto$cylinders, mpg)
plot(Auto$cylinders, Auto$mpg)
attach(Auto)
plot(Auto$cylinders, mpg)
cylinders = as.factor(cylinders)
plot(Auto$cylinders, mpg)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
hist(mpg)
hist(mpg, col=2)
hist(mpg, col=2, breaks=15)
pairs(Auto)
plot(horsepower, mpg)
identify(horsepower,mpg,name)
exit
abort
identify(horsepower,mpg,name)
summary(Auto)
summary(mpg)
?cd
??cd
View(College)
fix(College)
# rownames()
View(College)
college = read.table(College)
View(College)
fix(College)
?fix
edit(College)
edit(College)
?College
library(ISLR)
library(MASS)
?College
fix(College)
fix(College)
fix(College)
capabilities()
library(MASS)
install.packages("ISLR")
library(ISLR)
fix(Auto)
library(ISLR)
library(MASS)
fix(Auto)
fix(Auto)
# View(College)
# View(College)
edit(College)
fix(college)
fix(College)
College=College[,-1]
fix(College)
fix(College)
source('~/Dev/BEC/intro_to_statistical_learning/chpt2_statistical_learning/applied.R')
library(ISLR)
library(MASS)
rownames(College)=College[,1]
College=College[,-1]
fix(College)
View(College)
View(College)
college = read.csv("../data_sets/College.csv")
setwd("~/Dev/BEC/intro_to_statistical_learning")
college = read.csv("data_sets/College.csv")
fix(college)
library(ISLR)
library(MASS)
# 8
# (a) Use the read.csv() function to read the data into R
college = read.csv("data_sets/College.csv")
# (b) Look at the data using the fix() function. Note the first column is just the name of the university.
# We don't want R to treat this as data. Give each row a name corresponding to the appropriate university,
# so that R will not try to perform calculations on the row names. However, we still need to eliminate the first column
# in the data where the names are stored.
fix(college)
rownames(college)=college[,1]
fix(college)
college = college[,-1]
fix(college)
# 8(c)i. Use the summary() function to produce a numberical summary of the variables in the data set
summary(college)
?summary
# 8(c)i. Use the summary() function to produce a numberical summary of the variables in the data set
summary(college)
View(college)
?pairs
# 8(c)i. Use the summary() function to produce a numberical summary of the variables in the data set
summary(college)
# 8(c)ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables in the data
pairs(seq(1,10), college)
# 8(c)ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables in the data
pairs(1:10, college)
# 8(c)ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables in the data
pairs(p, college)
names(college)
# 8(c)ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables in the data
pairs(~ college$Private + college$Apps, college)
pairs(college)
summary(college)
rownames(college)=college[,1]
# 8(a) Use the read.csv() function to read the data into R
college = read.csv("data_sets/College.csv")
View(college)
rownames(college)=college[,1]
college = college[,-1]
# 8(c)i. Use the summary() function to produce a numberical summary of the variables in the data set
summary(college)
pairs(college)
pairs(college[1:4])
data.frame(college)
pairs(data.frame(college)[1:4])
pairs(data.frame(college))
?Auto
pairs(Auto)
pairs(college[2:4])
View(college)
fix(college)
summary(college)
# 8(c)ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables in the data
pairs(college[2:12])
dim(college)
plot(college$Outstate, college$Private)
# 8(c)iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private
college$Private = as.factor(college$Private)
summary(college)
plot(college$Outstate, college$Private)
# 8(c)iV. Create a new qualitative variable, called Elite, by 'binning' the Top10perc variable
# We are going to divide universities into two groups based on whether or not the proportion of students
# coming from the top 10% of their high school classes exceeds 50%
Elite=rep("No",nrow(college))
Elite[college$Top10perc > 50] = "Yes"
Elite = as.factor(Elite)
college=data.frame(college, Elite)
# Use the summary() function to see how many elite universities there are.
summary(college)
plot(college$Private, college$Outstate)
# Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.
plot(college$Elite, college$Outstate)
# Use the summary() function to see how many elite universities there are.
summary(college$Elite)
# 8(c)v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables.
# Hint; par(mfrow=c(2,2)) will divide the print window into four regions so that four plots can be made simultaneously.
# Modifying the arguments to this function will divide the screen in other ways
par(mfrow=c(2,2))
hist(college$Apps)
hist(college$Accept)
hist(college$Enroll)
hist(college$Grad.Rate)
library(ISLR)
library(MASS)
View(Boston)
names(Boston)
?Boston
attach(Boston)
lm.fit(medv~lstat)
attach(Boston)
lm.fit(medv~lstat)
lm.fit=lm(medv~lstat)
rm(list=ls())
library(ISLR)
library(MASS)
View(Boston)
names(Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
?detach
detach(Boston)
detach(Boston)
detach(Boston)
detach(Boston)
lm.fit=lm(medv~lstat)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
coefficients(lm.fit)
# Obtain a confidence interval for the coefficient estimates
confint(lm.fit)
# Produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="prediction")
# Plot medv and lstat along with the least squares regression line
plot(lstat, medv)
abline(lm.fit)
# Some other abline and plot examples
abline(lm.fit, lwd=3)
abline(lm.fit, lwd=3, col="red")
plot(lstat, medv, col="red")
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(1:20, 1:20, pch=1:20)
# View 4 diagnostic plots at once. Pass the output from lm.fit to plot(). Usually, you would have to press enter to cycle through them
par(mfrow=c(2,2))
plot(lm.fit)
# Compute residuals from a linear regression fit using the residuals() function.
# The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values
plot(predict(lm.fit), residuals(lm.fit))
# The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values
plot(predict(lm.fit), rstudent(lm.fit))
# On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can
# be computed for any number of predictors using the hatvalues() function
plot(hatvalues((lm.fit)))
# The which.max() function identifies the index of the largest element of a vector.
# In this case, it tells us which observation has the largest leverage statistic
which.max(hatvalues(lm.fit))
### Multiple Linear Regression
# Multiple linear regression with 2 predictors
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
# Using all the predictors in Boston dataset
lm.fit=lm(medv~., data=Boston)
summary(lm.fit)
?summary.lm
# Access individual components of summary object by name
summary(lm.fit)$r.sq
summary(lm.fit)$sigma  # RSE
?car
install.packages(car)
install.packages(car)
install.packages("car")
?car
# The vif() function (car packckage) can be used to compute variance inflation factors
library(car)
?car
?vif
vif(lm.fit)
# Perform regression using all variables but one
lm.fit1=lm(medv~.-age, data=Boston)
summary(lm.fit1)
# Or use update()
lm.fit1=update(lm.fit, ~.-age)
### Interaction Terms
# Include interaction terms using the syntax lstat:black
summary(lm(medv~lstat+age+lstat:age, data=Boston))
# Or use shorthand syntax
summary(lm(medv~lstat*age, data=Boston))
?lm
### Non-linear Transformations of the Predictors
# Perform a regression of medv onto lstat and lstat^2
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
# Note the near zero p-value associated with the quadratic term which suggests that it leads to an improved model
# We use anova() function to further quantify the extent to which the quadratic fit is superior
# to the linear fit
lm.fit=lm(medv~lstat)
anova(lm.fit, lm.fit2)
?arnova
?anova
anova(lm.fit, lm.fit2)
anova(lm.fit, lm.fit2)
anova(lm.fit2, lm.fit)
# anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the
# two models fit the data equally well, and the alternative hypothesis is that the full model is superior.
# Here the F-statistic is 135 and the associated p-value is virtually zero.
# This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far
# superior to the model that only contains the predictor lstat. This is not surprising, since earlier
# we saw evidence for non-linearity in the relationship between medv and lstat.
# If we do:
par(mfrow=c(2,2))
plot(lm.fit2)
plot(lm.fit)
plot(lm.fit2)
# we can see that when the lstat^2 term is included in the model, there is little discernable pattern
# in the residuals
# To create higher-order polynomials, use the following syntax
lm.fit5=lm.fit(medv~poly(lstat,5))
# we can see that when the lstat^2 term is included in the model, there is little discernable pattern
# in the residuals
# To create higher-order polynomials, use the following syntax
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
# This suggests that including additional polynomial terms, up to the fifth order, leads to an
# improvement in the model fit. However, further investigation of the data reveals that no polynomial
# terms beyond fifth order have significant p-values in a regression fit.
lm.fit7=lm(medv~poly(lstat,7))
summary(lm.fit7)
# We can also try a log transformation
summary(lm(medv~log(rm), data=Boston))
### Qualitative Predictors
# We will now examine the Carseats data and attempt to predict Sales (child car seat sales) in 400 locations
# based on a number of predictors.
View(Carseats)
names(Carseats)
# This data includes qualitative predictors such as Shelveloc, an indicator of the quality of the shelving
# location - that is, the space within a store in which the car seat is displayed - at each location.
# The predictor Shelveloc takes on three possible values, Bad, Medium, and Good. Given a qualitative
# variable such as Shelveloc, R generates dummy variables automatically.
# Below we fit a multiple regression model that includes some interaction terms.
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
# The contrasts() function returns the coding that R uses for the dummy variables
attach(Carseats)
contrasts(ShelveLoc)
### Writing Functions
LoadLibraries=function(){
library(ISLR)
library(MASS)
print("The libraries have been loaded!")
}
LoadLibraries()
LoadLibraries()
attach(Auto)
View(Auto)
# Simple linear regression with mpg as the response and horsepower as the predictor.
lm.fit=lm(mpg~horsepower)
# Use the summary() function to print the results and comment on the output.
summary(lm.fit)
# There is a relationship between predictor and response.
# The p-value is close to zero, therefor the relationship is strong
# The relationship between the predictor and the response is negative (coefficient is negative)
# What is the predicted mpg associated with a horsepower of 98?
predict(lm.fit, data.frame(horsepower=98))
# What are the associated 95% confidence and prediction intervals?
predict(lm.fit, data.frame(horsepower=98), interval="confidence")
predict(lm.fit, data.frame(horsepower=98), interval="prediction")
# Plot the response and the predictor.
# Use the abline()function to display the least squares regression line
plot(horsepower, mpg)
abline(lm.fit)
# Use the plot() function to produce diagnostic plots of the least squares regression fit.
# Comment on any problems you see with the fit.
par(mfrow=c(2,2))
plot(lm.fit)
## 8
# Multiple linear regression on the Auto data set
# Procude a scatterplot matrix which includes all of the variables in the data set.
pairs(Auto)
# Compute the matrix of correlations between the variables using the function cor().
# You will need to exclude the name variable, which is qualitative.
cor(subset(Auto, select=-name))
# Perform multiple linear regression with mpg as the response and all other
# variables except name as the predictors. Comment on the output.
lm.fit2 = lm(mpg~.-name, data=Auto)
summary(lm.fit2)
# Perform multiple linear regression with mpg as the response and all other
# variables except name as the predictors. Comment on the output.
lm.fit_multi = lm(mpg~.-name, data=Auto)
summary(lm.fit_multi)
# Use plot() function to produce diagnostic plots of the linear regression fit.
# Comment on any problems you see with the fit. Do the residual plots suggest any unusually
# large outliers? Does the leverage plot identify any observations with unusually hight leverage?
par(mfrow=c(2,2))
plot(lm.fit_multi)
# There is evidence of non-linearity
# Observation 14 has high leverage
# e) Use * and : symboles to fit linear regression models with interaction effects.
# Do any interactions appear to be statistically significant?
lm.fit_multi_1 = lm(mpg~displacement+weight+year+origin, data=Auto)
summary(lm.fit_multi_1)
lm.fit_multi_2 = lm(mpg~displacement+weight+year*origin, data=Auto)
summary(lm.fit_multi_2)
lm.fit_multi_3 = lm(mpg~displacement+origin+year*weight, data=Auto)
summary(lm.fit_multi_3)
lm.fit_multi_4 = lm(mpg~year+origin+displacement*weight, data=Auto)
summary(lm.fit_multi_4)
# All 3 interactions tested seem to have statistically significant effects.
# f) Try a few different transformations of the variables.
lm.fit_multi_5 = lm(mpg~poly(displacement,3)+weight+year+origin, data=Auto)
lm.fit_multi_6 = lm(mpg~displacement+I(log(weight))+year+origin, data=Auto)
lm.fit_multi_7 = lm(mpg~displacement+I(weight^2)+year+origin, data=Auto)
summary(lm.fit_multi_5)
summary(lm.fit_multi_6)
summary(lm.fit_multi_7)
