Power2 = function(x, a) {
print(x^a)
}
Power2(4, 3)
# c) Using Power2() function, compute 10^3, 8^17, 131^3.
Power2(10, 3)
Power2(8, 17)
Power2(131, 3)
# d) Now create a new function, Power3(), that actually returns the result x^a as an R object.
Power3 = function(x, a) {
return(x^a)
}
x = Power3(3, 3)
x
# e) Now using Power3() function, create a plot f(x)=x^2.
# The x-axis should display a range of integers from 1 to 10, and the y-axis should display x^2.
# Label the axes appropriately, and use an appropriate title for the figure. Consider displaying
# either the x-axis, the y-axis, or both on the log-scale.
x = 1:10
plot(x, Power3(x, 2), log="xy", ylab="Log of y = x^2", xlab="Log of x", main="Log of x^2 versus Log of x")
# f) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a
# fixed a and for a range of values of x. For instance, if you call PlotPower(1:10,3) then
# a plot should be created with an x-axis taking on values 1,2,...,10, and a y-axis on
# values 1^3, 2^3, ..., 10^3.
PlotPower = function(x, a) {
plot(x, Power3(x, a))
}
PlotPower(1:10,3)
## 13
# Using the Boston data set, fit classification models in norder to predict whether a given
# suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN
# models using various subsets of the predictors. Describe your findings.
library(MASS)
summary(Boston)
attach(Boston)
View(Boston)
crime01 = rep(0, length(crim))
crime01[crim > median(crim)] = 1
Boston = data.frame(Boston, crime01)
train = 1:(dim(Boston)[1]/2)
test = (dim(Boston)[1]/2 + 1):dim(Boston)[1]
Boston.train = Boston[train,]
Boston.test = Boston[test, ]
crime01.test = crime01[test]
# logistic regression
glm.fit = glm(crime01 ~ . - crime01 - crim, data=Boston, family=binomial, subset=train)
glm.probs = predict(glm.fit, Boston.test, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs>0.5] = 1
mean(glm.pred != crime01.test)
# LDA
lda.fit = lda(crime01~.-crime01-crim, data=Boston, subset=train)
lda.pred = predict(lda.fit, Boston.test)
mean(lda.pred$class != crime01.test)
lda.fit = lda(crime01~.-crime01-crim-chas-tax, data=Boston, subset=train)
lda.pred = predict(lda.fit, Boston.test)
mean(lda.pred$class != crime01.test)
lda.fit = lda(crime01~.-crime01-crim-chas-tax-lstat-indus-age, data=Boston, subset=train)
lda.pred = predict(lda.fit, Boston.test)
mean(lda.pred$class != crime01.test)
# KNN
library(class)
train.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[train, ]
test.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[test, ]
train.crime01 = crime01[train]
set.seed(1)
# KNN(k=1)
knn.pred = knn(train.X, test.X, train.crime01, k = 1)
mean(knn.pred != crime01.test)
# KNN(k=10)
knn.pred = knn(train.X, test.X, train.crime01, k=10)
mean(knn.pred != crime01.test)
# KNN(k=100)
knn.pred = knn(train.X, test.X, train.crime01, k=100)
mean(knn.pred != crime01.test)
# KNN(k=10) with subset of variables
train.X = cbind(zn, nox, rm, dis, rad, ptratio, black, medv)[train, ]
test.X = cbind(zn, nox, rm, dis, rad, ptratio, black, medv)[test, ]
knn.pred = knn(train.X, test.X, train.crime01, k=10)
mean(knn.pred != crime01.test)
source('~/Dev/BEC/intro_to_statistical_learning/chpt4_classification/chpt4_classification_applied.R')
rm(list=ls())
# Cross-Validation and the Bootstrap
## The Validation Set Approach
set.seed(1)
library(ISLR)
# Cross-Validation and the Bootstrap
## The Validation Set Approach
set.seed(1)
# Use sample() function to split the set of observations into two halves,
# by selecting a random subset of 196 observations out of the original 392 observations.
train=sample(392, 196)
# Then use the subset option in lm() to fit a linear regression using
# only the observations corresponding to the training set.
lm.fit = lm(mpg~horsepower, data=Auto, subset=train)
# Use predict() function to estimate the response for all 392 observations,
# and we use the mean() function to calculate the MSE of the 196 observations
# in the validation set. Note that the -train index below selects only the
# observations that are not in the training set.
attach(Auto)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
# Therefor the estimated MSE for the linear regression fit is 23.26.
# We can use the poly() function to estimate the test error for the quadratic
# and cubic regressions.
lm.fit2 = lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)
lm.fit3 = lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
# These errors are 18.72 and 18.79 respectively. If we choose a different training
# set instead, then we will obtain somewhat different errors on the validation set
set.seed(2)
train = sample(392, 196)
lm.fit = lm(mpg~horsepower, subset=train)
lm.fit = lm(mpg~horsepower, subset=train)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
lm.fit2 = lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)
lm.fit3 = lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
## Leave-One-Out Cross-Validation
# The LOOCV estimate can be automatically computed for any generalized linear
# model using the glm() and cv.glm() functions. In the lab for Chapter 4, we
# used the glm() function to perform logistic regression by passing in the
# family="binomial" argument. But if we use glm() to fit a model without
# passing in the family argument, then it performs linear regression, just
# like the lm() function. So for instance:
glm.fit = glm(mpg~horsepower, data=Auto)
coef(glm.fit)
# and
lm.fit = lm(mpg~horsepower, data=Auto)
coef(lm.fit)
# yield identical linear regression models. In this lab, we will perform linear regression
# using the glm() function rather than the lm() function because the former can be used together
# with cv.glm(). The cv.glm() function is part of the boot library.
library(boot)
glm.fit = glm(mpg~horsepower, data=Auto)
cv.err = cv.glm(Auto, glm.fit)
cv.err$delta
# The cv.glm() function produces a list with several components. The two numbers in the
# delta vector contain the cross-validation results. In this case the numbers are
# identical (up to two decimal places) and correspond to the LOOCV statistic given
# in (5.1). Below, we discuss a situation in which the two numbers differ.
# Our cross-validation estumate for the test error is approximately 24.23.
# We can repeat this procedure for increasingly complex polynomial fits. To automate
# the process, we use the for() function to initate a loop which iteratively fits
# polynomial regressions for polynomials of order i=1 to i=5, computes the associated
# cross-validation error, and stores it in the ith element of the vector cv.error.
# We begin by initializing the vector. This will probably take a few minutes
cv.error = rep(0, 5)
for (i in 1:5) {
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
## k-Fold Cross-Validation
# The cv.glm() function can also be used to implement k-fold CV. Below we use
# k=10, a common choice for k, on the Auto data set. We once again set a random
# seed and initialize a vector in which we will store the CV errors corresponding
# to the polunomial fits of orders one to ten.
set.seed(17)
cv.errors.10 = rep(0, 10)
cv.errors.10 = rep(0, 10)
for (i in 1:10) {
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10 = rep(0, 10)
for (i in 1:10) {
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
# in almost all situations. No complicated mathematical calculations are required.
# Performing a bootstrap analysis in R entails only two steps. First, we must create
# a function that computes the statistic of interest. Second, we use the boot() function,
# which is part of the boot library, to perform the bootstrap by repeatedly sampling
# observations from the data set with replacement.
# The Portfolio data set in the ISLR package is described earlier.
# To illustrate the use of the bootstrap on the data, we must first create
# a function, alpha.fn(), which takes as input (X, Y) data as well as a vector indication
# which observations should be used to estimate alpha. The function then outputs the
# estimate for alpha based on the selected observations,
alpha.fn = function(data, index) {
X = data$X[index]
Y = data$Y[index]
return ((var(Y) - cov(X, Y))/(var(X) + var(Y) - 2*cov(X, Y)))
}
# The function returns an estimate for alpha based on applying (5.7) to the obersvations
# indexed by the argument index. For instance, the following command tells R to estimate
# alpha using all 100 observations
alpha.fn(Portfolio, 1:100)
# The next command uses the sample() function to randomly select 100 observations from
# the range 1 to 100, with replacement. This is equivalent to constructin a new
# bootstrap data set and recomputing alpha estimate based on the new data set.
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
# We can implement a bootstrap analysis by performing this command many times,
# recording all of the corresponding estumates for alpha, and computing the resulting
# standard deviation. However, the boot() function automates this approach.
# Below we produce R=1000 bootstrap estimates for alpha.
boot(Portfolio, alpha.fn, R=1000)
# and predictions from a statistical learning method. Here we use the bootstrap approach
# in order to assess the variability of the estimates for B0 and B1, the intercept and slope
# terms for the linear regression model that uses horsepower to predict mpg in the Auto data
# set. We will compare the estimates obtained using the bootstrap to those obetained using
# the formulas for SE(B0 hat) and SE(B1 hat) described in Section 3.1.2.
# We first create a simple function, boot.fn(), which takes in the Auto data set as well
# as a set of indices for the observations, and returns the intercept and slope estimates for
# the linear regression model. We then apply this function to the full set of 392 observations
# in order to compute the estimates for B0 and B1 on the entire data set using the usual linear
# regression coefficient estimate formulas from Chapter 3.
boot.fn = function(data, index) {
return (coef(lm(mpg~horsepower, data=data, subset=index)))
}
boot.fn(Auto, 1:392)
# The boot.fn() function can also be used in order to create bootstrap estimates
# for the intercept and slope terms by randomly sampling from among the
# observations with replacement. Here we give two examples:
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
boot.fn(Auto, sample(392, 392, replace=T))
# Next, we use the boot() function to compute the standard errors of 1000 bootstrap estimates
# for the intercept and slope terms.
boot(Auto, boot.fn, 1000)
# This indicates that the bootstrap estimates for SE(B0 hat ) is 0.84, and that the bootstrap
# estimate for SE(B1 hat) is 0.0073. As discussed in Section 3.1.2, standard formulas can
# be used to compute the standard errors for the regression coefficients in a linear model.
# These can be obtained using summary() function.
summary(lm(mlg~horsepower, data=Auto))$coef
# This indicates that the bootstrap estimates for SE(B0 hat ) is 0.84, and that the bootstrap
# estimate for SE(B1 hat) is 0.0073. As discussed in Section 3.1.2, standard formulas can
# be used to compute the standard errors for the regression coefficients in a linear model.
# These can be obtained using summary() function.
summary(lm(mpg~horsepower, data=Auto))$coef
# in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and
# so the residuals from a linear fit will be inflated, and so will the noise variance. Secondly,
# the standard formulas assume (somewhat unrealisticaly) that the x_i are fixed, and all the
# variability comes from the variation in the errors e_i. The bootstrap approach does not
# rely on any of these assumptions, and so it is likely giving a more accurate estimate of
# the standard errors for B0 hat and B1 hat than the summary() function.
# Below we compute the bootstrap standard error estimates and the standard linear regression estimates
# that result from fitting the quadrati model to the data. Since this model provides a good
# fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates
# and the standard estimates of SE(B0 hat), SE(B1 hat), and SE(B2 hat).
boot.fn = function(data, index) {
coefficients(lm(mpg~horsepower+I(horsepower^2), data=data, subset=index))
}
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto)$coef
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
dd=read.csv("data.csv")
cd
setwd("~/Desktop/OneDrive - Queen's University/Queens/LOG6307/Labs/1_lab_git")
dd=read.csv("data.csv")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
rm(list=ls())
library(ISLR)
# 8) We will now perform cross-validation on a simulated data set.
# a) Generate a simulated data set as follows:
set.seed(1)
## 5
# In chapter 4, we used logistic regression to predict the probability of default using income
# and balance on the Default data set. We will now estimate the test error of this logistic
# regression model using the validation set approach. Do not forget to set a random seed before
# beginning your analysis.
summary(Default)
attach(Default)
# a) Fit a logistic regression model that uses income and balance to predict default
set.seed(1)
glm.fit = glm(default ~ income + balance, data = Default, family = binomial)
# b) Using the validation set approach, estaimte the test error of this model. In
# order to do this, you need to preform the following steps:
# i) Split the sample set into a training set and a validation set
# ii) Fit a multiple logistic regression model using only the training observations.
# iii) Obtain a prediction of default status for each individual in the validation set by
# computing the posterior probability of default for that individual, and classifying the individual
# to the default category if the posterior probability is greater than 0.5
# iv) Compute the validation set error, which is the fraction of the obervations in the validation
# set that are misclassified.
FiveB = function() {
# i.
train = sample(dim(Default)[1], dim(Default)[1]/2)
# ii.
glm.fit = glm(default ~ income + balance, data=Default, family=binomial, subset=train)
# iii.
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train, ], type = "response")
glm.pred[glm.probs > 0.5] = "Yes"
# iv.
return(mean(glm.pred != Default[-train, ]$default))
}
FiveB()
# c) Repeat the process in (b) three times, using three different splits of the observations into a
# training set and a validation set. Comment on the results obtained.
FiveB()
FiveB()
FiveB()
FiveB() #
# Average error is about  2.5%
# d) Now consider a logistic regression model that predicts the probability of default using income,
# balance, and a dummy variable for student. Estimate the test error for this model using the
# validation set approach. Comment on whether or not including a dummy variable for student leads to
# a reduction in the test error rate.
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default ~ income + balance + student, data=Default, family=binomial, subset=train)
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train, ], type = "response")
glm.pred[glm.probs > 0.5] = "Yes"
mean(glm.pred != Default[-train, ]$default)
# 6) We continue to consider the use of a logistic regression model to predict the probability of
# default using income and balance on the Default data set. In particular, we will now compute
# estimates for the standard errors of the income and balance logistic regression coefficients
# in two separate ways: (1) using the bootstrap, and (2) using the standard formula for computing
# the standard errors in the glm() function. Do not forget to set a random seed.
summary(Default)
# a) Using the summary() and glm() functions, determine the estimated standard errors for the
# coefficients associated with income and balance in a multiple logistic regression model that
# uses both predictors.
set.seed(1)
glm.fit = glm(default ~ income + balance, data=Default, family=binomial)
summary(glm.fit)
# b) Write a function, boot.fn(), that takes as input the Default data set as well as an index
# of observations, and that outputs the coefficient estimates for income and balance in the
# multiple logistic regression model.
boot.fn = function(data, index) {
return(coef(glm(default~income+balance, data=data, family=binomial, subset=index)))
}
# c) Use the boot() function together with your boot.fn() function to estimate the standard errors
# of the logistic regression coefficients for income and balance.
library(boot)
boot(Default, boot.fn, 50)
?boot
# 7) In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compare
# the LOOCV test error estimate. Alternatively, one would compute those quantities using
# just the glm() and predict.glm() functions, and a for loop. You will now take this approach
# in order to compute the LOOCV error for a simple logistic regression model on the Weekly
# data set. Recall that in the context of classification problems, the LOOCV error is given
# in (5.4)
summary(Weekly)
# 7) In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compare
# the LOOCV test error estimate. Alternatively, one would compute those quantities using
# just the glm() and predict.glm() functions, and a for loop. You will now take this approach
# in order to compute the LOOCV error for a simple logistic regression model on the Weekly
# data set. Recall that in the context of classification problems, the LOOCV error is given
# in (5.4)
summary(Weekly)
set.seed(1)
attach(Weekly)
# a) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly, family=binomial)
summary(glm.fit)
# b) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using
# all but the first observation.
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-1, ], family=binomial)
summary(glm.fit)
# c) Use the model from (b) to predict the direction of the first observations. You can
# do this by predicting that the first obervation will go up if
# P(Direction="Up" | Lag1, Lag2) > 0.5. Was this observation correctly classified?
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
# d) Write a for loop for i=1..n, where n is the number of observations in the data set,
# that performs each of the following steps:
# i) Fit a logistic regression model using all but he ith observation to predict Direction
# using Lag1 and Lag2.
# ii) Compute the posterior probability of the market moving up for the ith observation.
# iii) Use the posterior probability for the ith observation in order to predict whether or not
# the market moves up.
# iv) Determine whether or not an error was made in predicting the direction of the ith
# observation. If an error was made, then indicate this as a 1, and otherwise a 0.
count = rep(0, dim(Weekly)[1])
if (is_up != is_true_up) {
count[i] = 1
}
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = binomial)
is_up = predict.glm(glm.fit, Weekly[i, ], type = "response") > 0.5
is_true_up = Weekly[i, ]$Direction == "Up"
if (is_up != is_true_up) {
count[i] = 1
}
}
usum(count)
sum(count)
# c) Use the model from (b) to predict the direction of the first observation. You can
# do this by predicting that the first obervation will go up if
# P(Direction="Up" | Lag1, Lag2) > 0.5. Was this observation correctly classified?
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
# e) Take the average of the n numbers obtained in (d)iv in order to obtain the LOOCV
# estimate for the test error. Comment on the results.
mean(count)
# 8) We will now perform cross-validation on a simulated data set.
# a) Generate a simulated data set as follows:
set.seed(1)
x = rnorm(100)
y = x - 2*x^2 + rnorm(100)
# In this data set, what is n and what is p? Write out the model used to generate the data
# in equation form.
# n = 100, p = 2.
# Y= X − 2X^2 + ϵ.
# b) Create a scatterplot of X against Y. Comment on what you find.
plot(x, y)
# c) Set a random seed, and then compute the LOOCV errors that result from fitting the following
# four models using least squares:
library(boot)
Data = data.frame(x, y)
set.seed(1)
# i) Y = B0 + B1*X + epsilon
glm.fit = glm(y ~ x)
cv.glm(Data, glm.fit)$delta
# ii) Y = B0 + B1*X + B2*X^2 + epsilon
glm.fit = glm(y ~ poly(x, 2))
cv.glm(Data, glm.fit)$delta
# iii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + epsilon
glm.fit = glm(y ~ poly(x, 3))
cv.glm(Data, glm.fit)$delta
# iiii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + B4*X^4 + epsilon
glm.fit = glm(y ~ poly(x, 4))
cv.glm(Data, glm.fit)$delta
# i)
glm.fit = glm(y ~ x)
# Note you may find it helpful to use the data.frame() function to create a single data set
# containing both X and Y.
# d) Repeat (c) using another random seed, and report your results. Are your results the
# same as what you got in (c)? Why?
set.seed(10)
# i)
glm.fit = glm(y ~ x)
cv.glm(Data, glm.fit)$delta
# ii)
glm.fit = glm(y ~ poly(x, 2))
cv.glm(Data, glm.fit)$delta
# iii)
glm.fit = glm(y ~ poly(x, 3))
cv.glm(Data, glm.fit)$delta
# iv)
glm.fit = glm(y ~ poly(x, 4))
cv.glm(Data, glm.fit)$delta
# Exact same, because LOOCV will be the same since it evaluates n folds of a single observation.
# e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected?
# Explain your answer.
# The quadratic polynomial had the lowest LOOCV test error rate.
# This was expected because it matches the true form of Y.
# f) Comment on the statistical significance of the coefficient estimates that results
# from fitting each of the models in (c) using least squares. Do these results agree
# with the conclusions drawn based on the cross-validation results?
summary(glm.fit)
# 9) We will now consider the Boston housing data set, from the MASS library.
library(MASS)
summary(Boston)
set.seed(1)
attach(Boston)
# a) Based on this data set, provide an estimate for the population mean of medv. Call this
# estimate u_hat,
medv.mean = mean(medv)
medv.mean
# b) Provide an estimate of the standard error of u_hat. Interpret this result.
# Hint: We can compute the standard error of the sample mean by dividing the sample
# standard deviation by the square root of the number of observations.
medv.err = sd(medv)/sqrt(length(medv))
medv.err
# c) Now estimate the standard error of u_hat using the bootstrap. How does this compare
# to your answer from (b)?
boot.fn = function(data, index) {
return(mean(data[index]))
}
library(boot)
bstrap = boot(medv, boot.fn, 1000)
bstrap
# Similar to answer from (b) up to two significant digits. (0.4119 vs 0.4089)
# d) Based on your bootstrap estimate from (c), provide a 95% confidence interval for the
# mean of medv. Compare it to the results obtained using t.test(Boston$medv).
# Hint: You can approximate a 95% confidence interval using the formula
# [u_hat - 2SE(u_hat), u_hat + 2SE(u_hat)]
t.test(medv)
c(bstrap$t0 - 2 * 0.4106622, bstrap$t0 + 2 * 0.4106622)
# Bootstrap estimate very close to t.test estimate.
# e) Based on this data set, provide an estimate, u_hat_med, for the median value of medv in
# the population.
medv.med = median(medv)
medv.med
# f) We now would like to estimate the standard error of u_hat_med. Unfortunately, there is
# no simple formula for computing the standard error of the median. Instead, estimate the
# standard error of the median using the bootstrap. Comment on your findings.
boot.fn = function(data, index) {
return(median(data[index]))
}
boot(medv, boot.fn, 1000)
# Media of 21.1 and SE of 0.3770241. Small std. error relative to median value
# g) Based on this data set, provide an estimate for the tenth percentile of medv in
# Boston suburbs. Call this quantity u_hat_0.1. You can use the quantil() function.
medv.tenth = quantile(medv, c(0.1))
medv.tenth
# h) Use the bootstrap to estimate the standard error of u_hat_0.1. Comment on your findings.
boot.fn = function(data, index) {
return(quantile(data[index], c(0.1)))
}
boot(medv, boot.fn, 1000)
