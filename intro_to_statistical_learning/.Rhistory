cv.errors.10 = rep(0, 10)
cv.errors.10 = rep(0, 10)
for (i in 1:10) {
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10 = rep(0, 10)
for (i in 1:10) {
glm.fit = glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] = cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
# in almost all situations. No complicated mathematical calculations are required.
# Performing a bootstrap analysis in R entails only two steps. First, we must create
# a function that computes the statistic of interest. Second, we use the boot() function,
# which is part of the boot library, to perform the bootstrap by repeatedly sampling
# observations from the data set with replacement.
# The Portfolio data set in the ISLR package is described earlier.
# To illustrate the use of the bootstrap on the data, we must first create
# a function, alpha.fn(), which takes as input (X, Y) data as well as a vector indication
# which observations should be used to estimate alpha. The function then outputs the
# estimate for alpha based on the selected observations,
alpha.fn = function(data, index) {
X = data$X[index]
Y = data$Y[index]
return ((var(Y) - cov(X, Y))/(var(X) + var(Y) - 2*cov(X, Y)))
}
# The function returns an estimate for alpha based on applying (5.7) to the obersvations
# indexed by the argument index. For instance, the following command tells R to estimate
# alpha using all 100 observations
alpha.fn(Portfolio, 1:100)
# The next command uses the sample() function to randomly select 100 observations from
# the range 1 to 100, with replacement. This is equivalent to constructin a new
# bootstrap data set and recomputing alpha estimate based on the new data set.
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
alpha.fn(Portfolio, sample(100, 100, replace=T))
# We can implement a bootstrap analysis by performing this command many times,
# recording all of the corresponding estumates for alpha, and computing the resulting
# standard deviation. However, the boot() function automates this approach.
# Below we produce R=1000 bootstrap estimates for alpha.
boot(Portfolio, alpha.fn, R=1000)
# and predictions from a statistical learning method. Here we use the bootstrap approach
# in order to assess the variability of the estimates for B0 and B1, the intercept and slope
# terms for the linear regression model that uses horsepower to predict mpg in the Auto data
# set. We will compare the estimates obtained using the bootstrap to those obetained using
# the formulas for SE(B0 hat) and SE(B1 hat) described in Section 3.1.2.
# We first create a simple function, boot.fn(), which takes in the Auto data set as well
# as a set of indices for the observations, and returns the intercept and slope estimates for
# the linear regression model. We then apply this function to the full set of 392 observations
# in order to compute the estimates for B0 and B1 on the entire data set using the usual linear
# regression coefficient estimate formulas from Chapter 3.
boot.fn = function(data, index) {
return (coef(lm(mpg~horsepower, data=data, subset=index)))
}
boot.fn(Auto, 1:392)
# The boot.fn() function can also be used in order to create bootstrap estimates
# for the intercept and slope terms by randomly sampling from among the
# observations with replacement. Here we give two examples:
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
boot.fn(Auto, sample(392, 392, replace=T))
# Next, we use the boot() function to compute the standard errors of 1000 bootstrap estimates
# for the intercept and slope terms.
boot(Auto, boot.fn, 1000)
# This indicates that the bootstrap estimates for SE(B0 hat ) is 0.84, and that the bootstrap
# estimate for SE(B1 hat) is 0.0073. As discussed in Section 3.1.2, standard formulas can
# be used to compute the standard errors for the regression coefficients in a linear model.
# These can be obtained using summary() function.
summary(lm(mlg~horsepower, data=Auto))$coef
# This indicates that the bootstrap estimates for SE(B0 hat ) is 0.84, and that the bootstrap
# estimate for SE(B1 hat) is 0.0073. As discussed in Section 3.1.2, standard formulas can
# be used to compute the standard errors for the regression coefficients in a linear model.
# These can be obtained using summary() function.
summary(lm(mpg~horsepower, data=Auto))$coef
# in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and
# so the residuals from a linear fit will be inflated, and so will the noise variance. Secondly,
# the standard formulas assume (somewhat unrealisticaly) that the x_i are fixed, and all the
# variability comes from the variation in the errors e_i. The bootstrap approach does not
# rely on any of these assumptions, and so it is likely giving a more accurate estimate of
# the standard errors for B0 hat and B1 hat than the summary() function.
# Below we compute the bootstrap standard error estimates and the standard linear regression estimates
# that result from fitting the quadrati model to the data. Since this model provides a good
# fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates
# and the standard estimates of SE(B0 hat), SE(B1 hat), and SE(B2 hat).
boot.fn = function(data, index) {
coefficients(lm(mpg~horsepower+I(horsepower^2), data=data, subset=index))
}
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto)$coef
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
dd=read.csv("data.csv")
cd
setwd("~/Desktop/OneDrive - Queen's University/Queens/LOG6307/Labs/1_lab_git")
dd=read.csv("data.csv")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
dd=read.csv("data.csv")
dd$net=dd$nr_added_files-dd$nr_removed_files
dd$size=cumsum(dd$net)
plot(dd$size, type="l")
rm(list=ls())
library(ISLR)
# 8) We will now perform cross-validation on a simulated data set.
# a) Generate a simulated data set as follows:
set.seed(1)
## 5
# In chapter 4, we used logistic regression to predict the probability of default using income
# and balance on the Default data set. We will now estimate the test error of this logistic
# regression model using the validation set approach. Do not forget to set a random seed before
# beginning your analysis.
summary(Default)
attach(Default)
# a) Fit a logistic regression model that uses income and balance to predict default
set.seed(1)
glm.fit = glm(default ~ income + balance, data = Default, family = binomial)
# b) Using the validation set approach, estaimte the test error of this model. In
# order to do this, you need to preform the following steps:
# i) Split the sample set into a training set and a validation set
# ii) Fit a multiple logistic regression model using only the training observations.
# iii) Obtain a prediction of default status for each individual in the validation set by
# computing the posterior probability of default for that individual, and classifying the individual
# to the default category if the posterior probability is greater than 0.5
# iv) Compute the validation set error, which is the fraction of the obervations in the validation
# set that are misclassified.
FiveB = function() {
# i.
train = sample(dim(Default)[1], dim(Default)[1]/2)
# ii.
glm.fit = glm(default ~ income + balance, data=Default, family=binomial, subset=train)
# iii.
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train, ], type = "response")
glm.pred[glm.probs > 0.5] = "Yes"
# iv.
return(mean(glm.pred != Default[-train, ]$default))
}
FiveB()
# c) Repeat the process in (b) three times, using three different splits of the observations into a
# training set and a validation set. Comment on the results obtained.
FiveB()
FiveB()
FiveB()
FiveB() #
# Average error is about  2.5%
# d) Now consider a logistic regression model that predicts the probability of default using income,
# balance, and a dummy variable for student. Estimate the test error for this model using the
# validation set approach. Comment on whether or not including a dummy variable for student leads to
# a reduction in the test error rate.
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default ~ income + balance + student, data=Default, family=binomial, subset=train)
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train, ], type = "response")
glm.pred[glm.probs > 0.5] = "Yes"
mean(glm.pred != Default[-train, ]$default)
# 6) We continue to consider the use of a logistic regression model to predict the probability of
# default using income and balance on the Default data set. In particular, we will now compute
# estimates for the standard errors of the income and balance logistic regression coefficients
# in two separate ways: (1) using the bootstrap, and (2) using the standard formula for computing
# the standard errors in the glm() function. Do not forget to set a random seed.
summary(Default)
# a) Using the summary() and glm() functions, determine the estimated standard errors for the
# coefficients associated with income and balance in a multiple logistic regression model that
# uses both predictors.
set.seed(1)
glm.fit = glm(default ~ income + balance, data=Default, family=binomial)
summary(glm.fit)
# b) Write a function, boot.fn(), that takes as input the Default data set as well as an index
# of observations, and that outputs the coefficient estimates for income and balance in the
# multiple logistic regression model.
boot.fn = function(data, index) {
return(coef(glm(default~income+balance, data=data, family=binomial, subset=index)))
}
# c) Use the boot() function together with your boot.fn() function to estimate the standard errors
# of the logistic regression coefficients for income and balance.
library(boot)
boot(Default, boot.fn, 50)
?boot
# 7) In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compare
# the LOOCV test error estimate. Alternatively, one would compute those quantities using
# just the glm() and predict.glm() functions, and a for loop. You will now take this approach
# in order to compute the LOOCV error for a simple logistic regression model on the Weekly
# data set. Recall that in the context of classification problems, the LOOCV error is given
# in (5.4)
summary(Weekly)
# 7) In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compare
# the LOOCV test error estimate. Alternatively, one would compute those quantities using
# just the glm() and predict.glm() functions, and a for loop. You will now take this approach
# in order to compute the LOOCV error for a simple logistic regression model on the Weekly
# data set. Recall that in the context of classification problems, the LOOCV error is given
# in (5.4)
summary(Weekly)
set.seed(1)
attach(Weekly)
# a) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly, family=binomial)
summary(glm.fit)
# b) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using
# all but the first observation.
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-1, ], family=binomial)
summary(glm.fit)
# c) Use the model from (b) to predict the direction of the first observations. You can
# do this by predicting that the first obervation will go up if
# P(Direction="Up" | Lag1, Lag2) > 0.5. Was this observation correctly classified?
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
# d) Write a for loop for i=1..n, where n is the number of observations in the data set,
# that performs each of the following steps:
# i) Fit a logistic regression model using all but he ith observation to predict Direction
# using Lag1 and Lag2.
# ii) Compute the posterior probability of the market moving up for the ith observation.
# iii) Use the posterior probability for the ith observation in order to predict whether or not
# the market moves up.
# iv) Determine whether or not an error was made in predicting the direction of the ith
# observation. If an error was made, then indicate this as a 1, and otherwise a 0.
count = rep(0, dim(Weekly)[1])
if (is_up != is_true_up) {
count[i] = 1
}
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = binomial)
is_up = predict.glm(glm.fit, Weekly[i, ], type = "response") > 0.5
is_true_up = Weekly[i, ]$Direction == "Up"
if (is_up != is_true_up) {
count[i] = 1
}
}
usum(count)
sum(count)
# c) Use the model from (b) to predict the direction of the first observation. You can
# do this by predicting that the first obervation will go up if
# P(Direction="Up" | Lag1, Lag2) > 0.5. Was this observation correctly classified?
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
# e) Take the average of the n numbers obtained in (d)iv in order to obtain the LOOCV
# estimate for the test error. Comment on the results.
mean(count)
# 8) We will now perform cross-validation on a simulated data set.
# a) Generate a simulated data set as follows:
set.seed(1)
x = rnorm(100)
y = x - 2*x^2 + rnorm(100)
# In this data set, what is n and what is p? Write out the model used to generate the data
# in equation form.
# n = 100, p = 2.
# Y= X − 2X^2 + ϵ.
# b) Create a scatterplot of X against Y. Comment on what you find.
plot(x, y)
# c) Set a random seed, and then compute the LOOCV errors that result from fitting the following
# four models using least squares:
library(boot)
Data = data.frame(x, y)
set.seed(1)
# i) Y = B0 + B1*X + epsilon
glm.fit = glm(y ~ x)
cv.glm(Data, glm.fit)$delta
# ii) Y = B0 + B1*X + B2*X^2 + epsilon
glm.fit = glm(y ~ poly(x, 2))
cv.glm(Data, glm.fit)$delta
# iii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + epsilon
glm.fit = glm(y ~ poly(x, 3))
cv.glm(Data, glm.fit)$delta
# iiii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + B4*X^4 + epsilon
glm.fit = glm(y ~ poly(x, 4))
cv.glm(Data, glm.fit)$delta
# i)
glm.fit = glm(y ~ x)
# Note you may find it helpful to use the data.frame() function to create a single data set
# containing both X and Y.
# d) Repeat (c) using another random seed, and report your results. Are your results the
# same as what you got in (c)? Why?
set.seed(10)
# i)
glm.fit = glm(y ~ x)
cv.glm(Data, glm.fit)$delta
# ii)
glm.fit = glm(y ~ poly(x, 2))
cv.glm(Data, glm.fit)$delta
# iii)
glm.fit = glm(y ~ poly(x, 3))
cv.glm(Data, glm.fit)$delta
# iv)
glm.fit = glm(y ~ poly(x, 4))
cv.glm(Data, glm.fit)$delta
# Exact same, because LOOCV will be the same since it evaluates n folds of a single observation.
# e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected?
# Explain your answer.
# The quadratic polynomial had the lowest LOOCV test error rate.
# This was expected because it matches the true form of Y.
# f) Comment on the statistical significance of the coefficient estimates that results
# from fitting each of the models in (c) using least squares. Do these results agree
# with the conclusions drawn based on the cross-validation results?
summary(glm.fit)
# 9) We will now consider the Boston housing data set, from the MASS library.
library(MASS)
summary(Boston)
set.seed(1)
attach(Boston)
# a) Based on this data set, provide an estimate for the population mean of medv. Call this
# estimate u_hat,
medv.mean = mean(medv)
medv.mean
# b) Provide an estimate of the standard error of u_hat. Interpret this result.
# Hint: We can compute the standard error of the sample mean by dividing the sample
# standard deviation by the square root of the number of observations.
medv.err = sd(medv)/sqrt(length(medv))
medv.err
# c) Now estimate the standard error of u_hat using the bootstrap. How does this compare
# to your answer from (b)?
boot.fn = function(data, index) {
return(mean(data[index]))
}
library(boot)
bstrap = boot(medv, boot.fn, 1000)
bstrap
# Similar to answer from (b) up to two significant digits. (0.4119 vs 0.4089)
# d) Based on your bootstrap estimate from (c), provide a 95% confidence interval for the
# mean of medv. Compare it to the results obtained using t.test(Boston$medv).
# Hint: You can approximate a 95% confidence interval using the formula
# [u_hat - 2SE(u_hat), u_hat + 2SE(u_hat)]
t.test(medv)
c(bstrap$t0 - 2 * 0.4106622, bstrap$t0 + 2 * 0.4106622)
# Bootstrap estimate very close to t.test estimate.
# e) Based on this data set, provide an estimate, u_hat_med, for the median value of medv in
# the population.
medv.med = median(medv)
medv.med
# f) We now would like to estimate the standard error of u_hat_med. Unfortunately, there is
# no simple formula for computing the standard error of the median. Instead, estimate the
# standard error of the median using the bootstrap. Comment on your findings.
boot.fn = function(data, index) {
return(median(data[index]))
}
boot(medv, boot.fn, 1000)
# Media of 21.1 and SE of 0.3770241. Small std. error relative to median value
# g) Based on this data set, provide an estimate for the tenth percentile of medv in
# Boston suburbs. Call this quantity u_hat_0.1. You can use the quantil() function.
medv.tenth = quantile(medv, c(0.1))
medv.tenth
# h) Use the bootstrap to estimate the standard error of u_hat_0.1. Comment on your findings.
boot.fn = function(data, index) {
return(quantile(data[index], c(0.1)))
}
boot(medv, boot.fn, 1000)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
fix(Hitters)
names(Hitters)
# fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
library(leaps)
library(ISLR)
library(leaps)
install.packages("leaps")
library(leaps)
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
req.summary$rsq
reg.summary$rsq
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
plot(pressure)
par(mfrow=c(2, 2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="1")
plot(reg.summary$adr2, xlab="Number of Variables", ylab="Adjusted RSq", type="1")
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="1")
par(mfrow=c(2, 2))
reg.summary$rsq
par(mfrow=c(2, 2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="1")
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS",)
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type=1)
?plot
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
par(mfrow=c(2, 2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
plot(reg.summary$adr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
par(mfrow=c(2, 2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
which.max(reg.summary$adjr2)
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
par(mfrow=c(2, 2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
cp_min_index = which.min(reg.summary$cp)
points(cp_min_index, reg.summary$cp[cp_min_index], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
points(cp_min_index, reg.summary$cp[cp_min_index], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
cp_min_index = which.min(reg.summary$cp)
points(cp_min_index, reg.summary$cp[cp_min_index], col="red", cex=2, pch=20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
bic_min_index = which.min(reg.summary$bic)
points(bic_min_index, reg.summary$cp[bic_min_index], col="red", cex=2, pch=20)
bic_min_index = which.min(reg.summary$bic)
points(bic_min_index, reg.summary$cp[bic_min_index], col="red", cex=2, pch=20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(bic_min_index, reg.summary$cp[bic_min_index], col="red", cex=2, pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
which.min(reg.summary$bic)
points(6, reg.summary$cp[6], col="red", cex=2, pch=20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(6, reg.summary$cp[6], col="red", cex=2, pch=20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
which.min(reg.summary$cp)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
par(mfrow=c(2, 2))
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
par(mfrow=c(2, 2))
which.min(reg.summary$cp)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
par(mfrow=c(2, 2))
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="b")
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="b")
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full, 6)
par(mfrow=c(2, 2))
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir <- "/Users/BenRombaut/Desktop/Queens/CISC880/Labs/Lab2")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir <- "/Users/BenRombaut/Desktop/Queens/CISC880/Labs/Lab2")
bot_info <-
fread("Data/bot_info.csv", nThread=2)
install.packages("data.table")
library(data.table)
bot_info <-
fread("Data/bot_info.csv", nThread=2)
knitr::opts_knit$set(root.dir <- "/Users/BenRombaut/Desktop/Queens/CISC880/Labs/Lab2")
bot_info <-
fread("Data/bot_info.csv", nThread=2)
bot_info <-
o
bot_info <-
fread(root.dir + "Data/bot_info.csv", nThread=2)
root.dir
bot_info <-
fread(paste(root.dir, "Data/bot_info.csv"), nThread=2)
paste(root.dir, "Data/bot_info.csv")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir <- "/Users/BenRombaut/Desktop/Queens/CISC880/Labs/Lab2/")
library(data.table)
bot_info <-
fread("/Users/BenRombaut/Desktop/Queens/CISC880/Labs/Lab2/Data/bot_info.csv", nThread=2)
install.packages('IRkernel')
IRkernel::installspec()
jupyter labextension install @techrah/text-shortcuts
